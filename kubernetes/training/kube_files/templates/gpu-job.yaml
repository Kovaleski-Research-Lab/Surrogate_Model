# batch/v1 tells it to use the JOB API
apiVersion: batch/v1
# we are running a Job, not a Pod
kind: Job

# set the name of the job
metadata:
  name: training

spec:
  # how many times should the system
  # retry before calling it a failure
  # backoffLimit: 0
  template:
    spec:
      # should we restart on failure
      # what containers will we need
      containers:
        # the name of the container
        - name: andy-train
          # the image: can be from any pubic facing registry
          image: docker.io/ctvqfq/bunny_ai:spie-1.2
          #image: docker.io/kovaleskilab/meep_ml:spie-1.2 
          # the working dir when the container starts
          #workingDir: /develop/code/
          # should Kube pull it
          #imagePullPolicy: IfNotPresent
          # we need to expose the port
          # that will be used for DDP
          #ports:
          #  - containerPort: 8880
          # setting of env variables
          #env:
          #  # which interface to use
          #  - name: NCCL_SOCKET_IFNAME
          #    value: eth0
          #  # prints some INFO level
          #  # NCCL logs
          #  - name: NCCL_DEBUG
          #    value: INFO
          # the command to run when the container starts
          #command: ["python3", "surrogate_model/src/core/preprocess_data.py"]
          command: ["/bin/sh", "-c"]
          args: ["git clone https://github.com/Kovaleski-Research-Lab/surrogate_model.git .; cd /develop/code/surrogate_model/src/core; python3 /develop/code/surrogate_model/src/core/preprocess_data.py > /develop/data/spie_journal_2023/kube_logs/test.log"]
          #args: ["git clone https://github.com/Kovaleski-Research-Lab/surrogate_model.git .; cd /develop/code/src/core; python3 preprocess_data.py > /develop/data/spie_journal_2023/train_logs/test.log"]
          #args: ["git clone https://github.com/Kovaleski-Research-Lab/surrogate_model.git .; cd /develop/code/src/core/; touch /develop/data/spie_journal_2023/kube_logs/test.txt"]
          # define the resources for this container
          resources:
            # limits - the max given to the container
            limits:
              # RAM
              memory: 240G
              # cores
              cpu: 40
              # NVIDIA GPUs
              #nvidia.com/gpu: 1
            # requests - what we'd like
            requests:
              # RAM
              memory: 200G
              # CPU Cores
              cpu: 32
              # GPUs
              #nvidia.com/gpu: 1
          # what volumes should we mount
          volumeMounts:
            # my datasets PVC should mount to /data
            - mountPath: /develop/results
              name: meep-results
            # IMPORTANT: we need SHM for DDP (shared mem, distributed data parallel)
            #- mountPath: /dev/shm
            #  name: dshm
      # tell Kube where to find the volumes we want to use
      volumes:
        # which PVC is my data
        - name: meep-results
          persistentVolumeClaim:
            claimName: meep-results
      restartPolicy: Never
        # setup shared memory as a RAM volume
        #- name: dshm
        #  emptyDir:
        #    medium: Memory
      # Tell Kube what type of GPUs we want
      #affinity:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #        - matchExpressions:
      #            - key: nvidia.com/gpu.product
      #              operator: In
      #              values:
      #                # asking for 3090s only
      #                - NVIDIA-GeForce-RTX-3090
